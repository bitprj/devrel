{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LAX': 898, 'YVR': 449, 'JFK': 449}\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"flight_data.csv\", \"r\")\n",
    "\n",
    "counts = {\"LAX\": 0, \"YVR\": 0, \"JFK\": 0}\n",
    "for line in data_file:\n",
    "    airport = line[:3]\n",
    "    if airport in counts.keys():\n",
    "        counts[airport] += 1\n",
    "\n",
    "data_file.close()\n",
    "    \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Driven Decisions\n",
    "\n",
    "In this piece, we'll get down to the core of what data means, how we can extract knowledge from information from data, and how to use it as a savvy businessperson, researcher, or analyst. You won't need any preparation, but there will be some code available for the aspiring programmers to use and modify. You will walk away from this page with a better understanding of what your data needs or needs not and how to most effectively to use it.\n",
    "\n",
    "\n",
    "\n",
    "# Part 1: Information vs Data\n",
    "\n",
    "Webster's online dictionary defines Data as factual measurements or statistics used as a basis for actions such as reasoning, deliberations, or calculation. We want to pay specific attention to the word \"basis\" here, as it highlights the fact that data can be, quite easily, manipulated. If we don't want to be susceptible to this manipulation, then we should make sure we turn that data into information. We see that Information's entry is \"knowledge obtained from investigation, study, or instruction,\" and that is really what's at play when we're making smart, data-driven decisions:\n",
    "\n",
    "## Fallacies and Manipulation\n",
    "\n",
    "<!--  --> cite: https://www.datasciencecentral.com/profiles/blogs/data-fallacies-to-avoid-an-illustrated-collection-of-mistakes\n",
    "\n",
    "The first step in being a conscious member of the data community is to identify and refute bad data appropriately. There are many resources for the many, varied ways to manipulate data, but we'll look at only the most common and most relevant ones here.\n",
    "\n",
    "#### Confirmation Bias\n",
    "\n",
    "The biggest consequence (good or bad) of the information age, is that data is available at the click of a mouse or wake-word in almost everybody's homes. It certainly is good for us to have access to more, but that means that we now have to be aware of confirmation bias as we interpret this data. If you're looking for the answer to a question that you don't have an opinion on, even slight differences in phrasing will affect the content of your results. Take Coffee and Acne for example, in a simple Google search question we see very different results based on the choice of cause/cure:\n",
    " \n",
    " Cure            |  Cause\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://i.imgur.com/z1FkMNB.png)  |  ![](https://i.imgur.com/qIPZvc2.png)\n",
    "\n",
    "We see that even in a case where you're not trying to put your bias into the search, that your results can be skewed. This can be resolved by looking at aggregates of data instead of single-sources. This makes way for \"pop science\" where a study finds that X causes/cures Y, where both X and Y are common conditions. We see more how these unfortunate statistical anomalies become daytime news stories here.\n",
    "\n",
    "\n",
    "#### Data Dredging\n",
    "\n",
    "Data dredging is the effect when intentional or not, you have too many variables or opportunities for correlation for a single dataset. There is a famous example of this, when the website FiveThirtyEight reported on the specific field of nutrition, as the data available has issues already. They noted the specific issues of how they generate \"huge data sets with many, many variables,\" which are the most susceptible data dredging. [The article](https://fivethirtyeight.com/features/you-cant-trust-what-you-read-about-nutrition/) finds a few particularly remarkable relationships, where they found the following links from the respondent data:\n",
    "\n",
    "![](https://i.imgur.com/CoKDCuc.png)\n",
    "\n",
    "You'll (hopefully) not be surprised to know that there is no correlation in real life between cabbage and innie bellybuttons or tomatoes and Judaism. This is a function, as the article explains, of over 27,000 regressions being used over less than 60 complete responses, where they expect a 5% false-positive rate. Why 5%? well, that goes into the next topic, p-hacking.\n",
    "\n",
    "\n",
    "#### P-Hacking and Cherry Picking\n",
    "\n",
    "You might have noticed the far-right column in the above picture has a \"p-value\" label. This is the value that statisticians use to measure the risk of a false positive. In general, studies with a p-value of less than 0.05, or 5%, can be published. It is a hard burden to prove in some situations, but in some situations it is absurdly easy. Linked in the above article is an [interactive piece](https://projects.fivethirtyeight.com/p-hacking/) by FiveThirtyEight that allows you to use p-hacking in action. This one comes with a bonus of letting you lean into confirmation bias as well, as it contains economic and political data from the last ~70 years. You get to cherry-pick the types of political data to use and the ways to measure a \"good\" economy, and the right panel allows you to see how significant your results are. It should go without saying that if you cherry-pick enough, you can prove any conclusion you want to.\n",
    "\n",
    "This phenomenon is very possible in the transfer of information at any stage from data sources to scientists to reporters to consumers, again intentional or unintentional. We see this taken to a comedic level with XKCD's exploration of jelly beans:\n",
    "\n",
    "![](https://i.imgur.com/k2dDf9e.png)\n",
    "\n",
    "\n",
    "What you should take away here is that data manipulated is not meaningful, and I argue it is not even information, as it does not represent knowledge in any real way. Let's not be too cynical now, there _is_ meaningful information in the world, and we want to separate it from this darker part of the data sphere.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Meaningful Information\n",
    "\n",
    "Here we have information, not misleading or malicious, and want to determine what to do with it. So then, what makes your information worth producing and using?\n",
    "\n",
    "#### \"Useful\" Information\n",
    "\n",
    "When we say \"useful\", what we want is SMART. SMART is an acronym that is generally used for goal-setting, where it stands for:\n",
    "\n",
    "- S: Specific\n",
    "- M: Measurable\n",
    "- A: Attainable\n",
    "- R: Relevant\n",
    "- T: Timely\n",
    "\n",
    "<!--  --> cite https://www.mindtools.com/pages/article/smart-goals.htm\n",
    "\n",
    "These are the things that you want in goal setting for sure, but we also want them in information, and it's no coincidence (p > 0.05). We want information that supports the goals you have when using it. Otherwise, information is as good as trivia, which is not bad necessarily, but useless by definition. Very briefly point by point:\n",
    "\n",
    "- Specific: this information should be scoped to answer the question(s) you have, as information too broad risks being less applicable to the situation\n",
    "- Measurable: an excellent metric for what to do with your data, and the subject of the next part\n",
    "- Attainable: similar to specificity, information too specific might be too difficult to build a solution for, or not worth the development time\n",
    "- Relevant: if your information is not about your subject, it won't be a good basis for solutions\n",
    "- Timely: while data being too old doesn't mean that it's wrong, it makes the correlation to today weaker and harder to build for. Likewise, a projection too far in the future might not be appropriate for your current project or plans.\n",
    "\n",
    "\n",
    "#### \"Strong\" Information\n",
    "\n",
    "While this sounds like a dataset for powerlifting competitions, it reflects the next stage in data-driven decision making that we need to address, what do we do with the data?\n",
    "\n",
    "Consider the following information:\n",
    "> Men 18-25 years old in the last 6 months were more likely to purchase jeans than men aged 25-40. \n",
    "\n",
    "What does a company that produces jeans do with this information? Probably they would increase brand marketing to men who are 18-25, but by how much? The information makes a distinction (more likely vs less likely), not a measurement (X percent more likely). If the information was instead:\n",
    "> Men 18-25 years old in the last 6 months were 10 times more likely to purchase jeans than men aged 25-40.\n",
    "\n",
    "Ad agencies would be jumping from their seats to start hiring male actors between 18 and 25! This touches on how useful information can be, and, as a bonus, is generally a good way to tell how good the question being asked in a survey was.\n",
    "\n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "On our journey to data-enlightenment, we do need to start somewhere. I've enjoyed sharing comics and other cherry-picked factoids for you all, but let's now practice what we preach.\n",
    "\n",
    "#### Covid-19 data\n",
    "\n",
    "The ongoing pandemic is something that has had lots of impact on the world, as I'm sure you have already noticed. The ways that it has had an impact is measurable, both to the human body and to our human society, and many high-stakes decisions depend on understanding this data. The sources online are variant, with online repositories like [Kaggle](https://www.kaggle.com/datasets) and [Google Research](https://datasetsearch.research.google.com/) and official sources like [data.gov](http://www.data.gov) and the [CDC](https://www.cdc.gov/datastatistics/index.html).\n",
    "\n",
    "There are many data visualizations and interactives as well as experts scrutinizing and summarizing this data for us all, thankfully. If you are looking for a kit project or want to practice with some visualization tools, however, this might be a good place to start working.\n",
    "\n",
    "One of these that is particularly well-made is the [arcgis dashboard](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6), which aggregates data and allows you to see the impact on your local county as well as the national/international trends. It does this without leaving room for p-hacking by allowing inappropriate filtering or manipulation, but still answering the questions it seeks to address. You can see the trendline for every state, as well as any country by selecting it in the list on the left. See here:\n",
    "\n",
    "![COVID](https://i.imgur.com/KGWmlY8.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### Census data\n",
    "\n",
    "The founding fathers of America were many things, but statisticians they were not. They did not have a particular fascination with regression, machine learning models, or gradient descent, and we shall shame them for that. However, they did appreciate the value of good data, err, data at least. From Article 1, section 2 of the U.S. Constitution:\n",
    "\n",
    "> Representatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers, which shall be determined by adding to the whole Number of free Persons, including those bound to Service for a Term of Years, and excluding Indians not taxed, three fifths of all other Persons. The actual Enumeration shall be made within three Years after the first Meeting of the Congress of the United States, and within every subsequent Term of ten Years, in such Manner as they shall by Law direct.\n",
    "\n",
    "<!-- Cite: https://www.census.gov/history/pdf/Article_1_Section_2.pdf -->\n",
    "\n",
    "<!-- I don't want to make historical judgments on the moral quality of this data, but the statistical quality of the data is prevalent here. We see that there is 10-year periodic data-gathering, a full count of \"free Persons\", and a multiple on \"all other Persons.\" This means that the ratification of the 14th amendment (that rescinded the three-fifths compromise) in 1868 should be measurable in the differences in the 1860 and 1870 censuses.\n",
    "\n",
    "I DID NOT SEE THE CORRELATION I SUPPOSED HERE... I WILL INVESTIGATE FURTHER, BUT THIS SEEMS LIKE AN INTERESTING FOLLOW UP ARTICLE IF THE MYSTERY IS RESOLVED IN AN INTERESTING WAY-->\n",
    "\n",
    "What all of that text means is that every 10 years, falling on the years ending in 0, there is a nationwide tally of the people who live in the country. There is some historical context here of course as well, including the exception to Indian reservations that are not _really_ part of the U.S. in lots of ways, and the three-fifths compromise that was removed in the 14th amendment.\n",
    "\n",
    "So what do we do with this? Well, there is a tally of people living in the U.S. since its founding, and that data is aggregated and made available to the public. The individual entries are removed to protect people's privacy, but the country is big enough that the aggregates can often be quite granular. A nice repository of information is [available online](https://www.census.gov/population/www/censusdata/hiscendata.html) and is used every day for things like redistricting, allocating federal funds, and business decisions. The last of these is probably most applicable to us here today, as the types of business decisions feed us into the types of conclusions we want to draw from this information, turning knowledge into conclusions.\n",
    "\n",
    "\n",
    "#### Flight data\n",
    "\n",
    "In the following part, we will work with a dataset I have constructed specifically to make both high- and low-level decisions, as well as cleaning a dataset and detecting anomalies. This is not a real dataset, as will become clear, but hopefully, it is large enough for meaningful analysis and simple enough for interpretation with the real-world correlations.\n",
    "\n",
    "\n",
    "\n",
    "# Part 2: Knowledge into Decisions\n",
    "\n",
    "\n",
    "#### Flight data, continued\n",
    "\n",
    "This dataset is made to represent flights leaving from SFO over a given time period, with info about the destination airport, the number of seats on the plane, a field called \"occupied\", and a time guage. We see the first few lines of the dataset here:\n",
    "\n",
    "`AIRPORT,SEATS,OCCUPIED,TIME`\n",
    "`LAX,200,180,WEEKDAY`\n",
    "`JFK,200,161,WEEKDAY`\n",
    "`LAX,200,186,WEEKDAY`\n",
    "`YVR,200,0.8315980689983522,WEEKDAY`\n",
    "`LAX,250,227,REDEYE`\n",
    "`JFK,250,226,REDEYE`\n",
    "`LAX,250,228,REDEYE`\n",
    "`YVR,250,0.8663797750002538,REDEYE`\n",
    "&vellip;\n",
    "\n",
    "We will access the full 1800 line file in the code, but all of the contents will follow the format above.\n",
    "\n",
    "\n",
    "## Types of Conclusions\n",
    "\n",
    "#### What makes a high-level conclusion?\n",
    "\n",
    "These types of conclusions are for very broad generalizations and are mostly observations, rather than predictions. There is a way to categorize the types of insights gained from data analysis: Descriptive, Diagnostic, Predictive, and Prescriptive. These can be thought of as increasing both specificities and work required: \n",
    "\n",
    "![](https://i.imgur.com/HlYuOwH.png)\n",
    "\n",
    "<!-- cite: https://www.scnsoft.com/blog/4-types-of-data-analytics -->\n",
    "\n",
    "A high-level conclusion is then one of the first two of these and is broader in scope. For example, in looking at the flight data we see that there are twice as many flights to LA relative to Vancouver or New York. This information is probably easy to see, and we can confirm it over the entire dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other similar high-level conclusions might be if the size of the planes is related to the destinations if the occupancy is related to the plane sizes alone, etc. Most high-level conclusions can be drawn from the data directly, provided you are aware of the potential fallacies and have verified your data.\n",
    "\n",
    "Speaking of verifying your data, why is the occupied field for the YVR flights a decimal value instead of an integer number of seats like in the LAX and JFK flights? Well that would be something we have to reason about. It would seem that the Canadian flight logs record percentage of the seats occupied instead of the number of seats occupied. Not fixing this would present a barrier to analyzing the data much further, and indeed should be fixed asap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "data_file = open(\"flight_data.csv\", \"r\")\n",
    "cleaned_data = open(\"cleaned_flight_data.csv\", \"w\")\n",
    "\n",
    "for line in data_file:\n",
    "    airport = line[:3]\n",
    "    if airport == \"YVR\":\n",
    "        fields = line.split(\",\")\n",
    "        num_seats = int(fields[1])\n",
    "        occ_percent = float(fields[2])\n",
    "        occ_count = floor(num_seats * occ_percent)\n",
    "        line = \"{},{},{},{}\".format(fields[0], fields[1], occ_count, fields[3])\n",
    "        \n",
    "    cleaned_data.write(line)\n",
    "    \n",
    "data_file.close()\n",
    "cleaned_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that any high-level conclusion is an observation more than it is analysis. It necessarily does not try to claim some new data coming in or hypotheticals, those would be in the next section.\n",
    "\n",
    "\n",
    "#### What makes a specific conclusion?\n",
    "\n",
    "While it would feel natural to draw a dichotomy here, it would be inappropriate to do so. Low-level or specific conclusions depend highly on the high-level conclusions to make prescriptive or prospective claims. The difference between these two is that prescriptive is advocating for a specific next action, the way we might want for making good, data-driven decisions, while prospective is claiming what is likely to happen. Understand that the difficulty with prescription is causality, that doing one thing would cause another when prescriptive simply acknowledges a connection of some kind. We go back to XKCD to explore another application of bad statistical thinking:\n",
    "\n",
    "![](https://i.imgur.com/zvdeUAn.png)\n",
    "\n",
    "<!-- cite: https://www.explainxkcd.com/wiki/index.php/687:_Dimensional_Analysis -->\n",
    "\n",
    "Of course, despite the ironclad logic, this is wrong. We see that the correlation is present, but we understand from a prospective analysis that there isn't a dependence between any of the variables. Professor Cueball then asserts that Toyota's engineers affect the geology of western Europe, which is yet to be proven. \n",
    "\n",
    "Alright, so we have some flight data, but so what? Well, the businesspeople upstairs have some new investment money that they would like to invest in the airport, financing some additional flights to one of these three destinations. They care about getting the most people through the airport to increase sales of day-old Auntie Anne's. Where would you assign this flight? The weekend to JFK? A redeye to LAX? We have to prescribe something, so it might be worth looking at the data more closely to make the best recommendation.\n",
    "\n",
    "Let's start with a look at the occupancy rates for all of the specific flights, that is every combo of airport and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAX_WEEKDAY\n",
      " 0.8508\n",
      "JFK_WEEKDAY\n",
      " 0.83\n",
      "YVR_WEEKDAY\n",
      " 0.8165\n",
      "LAX_REDEYE\n",
      " 0.8531\n",
      "JFK_REDEYE\n",
      " 0.8807\n",
      "YVR_REDEYE\n",
      " 0.8192\n",
      "LAX_WEEKEND\n",
      " 0.8508\n",
      "JFK_WEEKEND\n",
      " 0.8307\n",
      "YVR_WEEKEND\n",
      " 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_file = open(\"cleaned_flight_data.csv\", \"r\")\n",
    "\n",
    "data_by_airport_time = {}\n",
    "for line in cleaned_data_file:\n",
    "    fields = line.split(\",\")\n",
    "    airport = fields[0]\n",
    "    if airport == \"AIRPORT\":\n",
    "        continue\n",
    "    time = fields[3]\n",
    "    \n",
    "    key = \"{}_{}\".format(airport, time)\n",
    "    if key not in data_by_airport_time.keys():\n",
    "        data_by_airport_time[key] = []\n",
    "    \n",
    "    occupancy_percent = int(fields[2]) / int(fields[1])\n",
    "    data_by_airport_time[key].append(occupancy_percent)\n",
    "    \n",
    "cleaned_data_file.close()\n",
    "\n",
    "rate_by_airport_time = {}\n",
    "for key, data_arr in data_by_airport_time.items():\n",
    "    occupancy_average = sum(data_arr) / len(data_arr)\n",
    "    rate_by_airport_time[key] = occupancy_average\n",
    "\n",
    "[print(k, round(v, 4)) for k,v in rate_by_airport_time.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! We can now make a conclusion about which flights would be best to add to the current schedule. From the looks of it, another redeye to JFK would be the best bet here, but this relies on the assumption that the additional supply would produce additional demand. If the demand is static for redeye flights, meaning the supply changing has little effect on the demand, then adding flight here would do very little. It is also possible that the current flights, even though they are the most full, do not have any additional demand for additional seats. These aren't questions we can answer with this dataset alone but might be one you could get from somebody else working at the airport.\n",
    "\n",
    "\n",
    "#### Robot conclusions\n",
    "\n",
    "Finally, we can acknowledge that machine learning and artificial intelligence exist. Sometimes, the data-driven decisions we want to be made are not those we make ourselves, but those that we train our robot friends to make for us. There are many considerations for training a machine learning model or informing an AI's guiding parameters, and they are, unfortunately, mostly out of the scope of this piece. It is noteworthy, however, that the same issues that plague human decision-makers are prevalent with machines too. There are also additional artifacts of the mathematical tools that built these machines causing additional strife, such as local minima for gradient descent and overfitting. These things must all be kept in mind when deciding where to feed the output of your ML model.\n",
    "\n",
    "\n",
    "\n",
    "# Part 3: Takeaways\n",
    "\n",
    "## Data Visualizations\n",
    "\n",
    "There is a huge elephant in the room here: sometimes data needs to be communicated from the analyst to other people. The analyst might be the one making decisions and might not be, and the decision-maker might be data conscious and might not be, so how do we bridge that gap? An extensive blog post about making data-driven decisions? NO! Data visualizations of course!\n",
    "\n",
    "There are many ways to communicate the conclusions that you have gathered, and there are many ways that those conclusions can be presented. We want to make sure that in our effort to convince, we're not stepping over the line of manipulating the other person. There are some heinous examples of misleading visualizations, some of the worst posted online and easy to search up in [aggreator lists](https://analythical.com/blog/examples-of-awful-data-visualization). It is worth taking some time to see the ways these can be misleading too, as the common mistakes can creep into your visualizations too if you're not careful. I've found [this guide from venngage](https://venngage.com/blog/misleading-graphs/) quite useful in familiarizing myself with the do not's of data viz.\n",
    "\n",
    "And finally, the complete guide, a bit old and extensive but still incredibly relevant as a resource: [The Quartz guide to bad data](https://qz.com/572338/the-quartz-guide-to-bad-data/). This piece is **HUGE**, but if you're looking for something and can't find it anywhere else, it likely lives in the tomes of Quartz.\n",
    "\n",
    "\n",
    "## Resource and Reference Links\n",
    "\n",
    "I've aggregated the links to most resources throughout the article in order here, for reference, or for further exploration, which I would highly encourage for any aspiring data scientists or teachers among you.\n",
    "\n",
    "- https://www.datasciencecentral.com/profiles/blogs/data-fallacies-to-avoid-an-illustrated-collection-of-mistakes\n",
    "- https://www.merriam-webster.com/dictionary/data\n",
    "- https://www.merriam-webster.com/dictionary/information\n",
    "- https://www.google.com/search?q=does+coffee+cause+acne&rlz=1C5CHFA_enUS893US893&oq=does+coffee+cause+acne&aqs=chrome..69i57j0l7.8121j0j7&sourceid=chrome&ie=UTF-8\n",
    "- https://www.google.com/search?q=does+coffee+cure+acne&rlz=1C5CHFA_enUS893US893&oq=does+coffee+cure+acne&aqs=chrome..69i57.2119j0j7&sourceid=chrome&ie=UTF-8\n",
    "- https://fivethirtyeight.com/features/you-cant-trust-what-you-read-about-nutrition/\n",
    "- https://fivethirtyeight.com/features/science-isnt-broken/#part1\n",
    "- https://projects.fivethirtyeight.com/p-hacking/\n",
    "- https://www.explainxkcd.com/wiki/index.php/882:_Significant\n",
    "- https://www.mindtools.com/pages/article/smart-goals.htm\n",
    "- https://www.census.gov/history/pdf/Article_1_Section_2.pdf\n",
    "- https://www.census.gov/population/www/censusdata/hiscendata.html\n",
    "- https://2020census.gov/en/census-data.html\n",
    "- https://www.scnsoft.com/blog/4-types-of-data-analytics\n",
    "- https://www.explainxkcd.com/wiki/index.php/687:_Dimensional_Analysis\n",
    "- https://analythical.com/blog/examples-of-awful-data-visualization\n",
    "- https://qz.com/572338/the-quartz-guide-to-bad-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
